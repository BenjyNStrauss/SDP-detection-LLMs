/*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*    http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing,
* software distributed under the License is distributed on an
* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
* KIND, either express or implied.  See the License for the
* specific language governing permissions and limitations
* under the License.
*/
package org.apache.cassandra.utils;

import java.util.Iterator;
import java.util.NoSuchElementException;

import com.google.common.collect.PeekingIterator;

public abstract class AbstractIterator<V> implements Iterator<V>, PeekingIterator<V>, CloseableIterator<V>
{

    private static enum State { MUST_FETCH, HAS_NEXT, DONE, FAILED }
    private State state = State.MUST_FETCH;
    private V next;

    protected V endOfData()
    {
        state = State.DONE;
        return null;
    }

    protected abstract V computeNext();

    public boolean hasNext()
    {
        switch (state)
        {
            case MUST_FETCH:
                state = State.FAILED;
                next = computeNext();

            default:
                if (state == State.DONE)
                    return false;

                state = State.HAS_NEXT;
                return true;

            case FAILED:
                throw new IllegalStateException();
        }
    }

    public V next()
    {
        if (state != State.HAS_NEXT && !hasNext())
            throw new NoSuchElementException();

        state = State.MUST_FETCH;
        V result = next;
        next = null;
        return result;
    }

    public V peek()
    {
        if (!hasNext())
            throw new NoSuchElementException();
        return next;
    }

    public void remove()
    {
        throw new UnsupportedOperationException();
    }

    public void close()
    {
        //no-op
    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.cassandra.repair.state;

import java.util.EnumMap;

import org.apache.cassandra.utils.Clock;

public abstract class AbstractState<T extends Enum<T>, I> extends AbstractCompletable<I> implements State<T, I>
{
    protected enum UpdateType
    {
        NO_CHANGE, ACCEPTED,
        LARGER_STATE_SEEN, ALREADY_COMPLETED;

        protected boolean isRejected()
        {
            switch (this)
            {
                case NO_CHANGE:
                case ACCEPTED:
                    return false;
                case LARGER_STATE_SEEN:
                case ALREADY_COMPLETED:
                    return true;
                default:
                    throw new IllegalStateException("Unknown type: " + this);
            }
        }
    }

    public static final int INIT = -1;
    public static final int COMPLETE = -2;

    private final Class<T> klass;
    protected final long[] stateTimesNanos;
    protected int currentState = INIT;

    public AbstractState(Clock clock, I id, Class<T> klass)
    {
        super(clock, id);
        this.klass = klass;
        this.stateTimesNanos = new long[klass.getEnumConstants().length];
    }

    @Override
    public boolean isAccepted()
    {
        return currentState == INIT ? false : true;
    }

    @Override
    public T getStatus()
    {
        int current = currentState;
        if (current < 0) // init or complete
            return null;
        return klass.getEnumConstants()[current];
    }

    public String status()
    {
        T state = getStatus();
        Result result = getResult();
        if (result != null)
            return result.kind.name();
        if (state == null)
            return "init";
        return state.name();
    }

    @Override
    public String toString()
    {
        return getClass().getSimpleName() + "{" +
               "id=" + id +
               ", status=" + status() +
               ", lastUpdatedAtNs=" + lastUpdatedAtNs +
               '}';
    }

    public int getCurrentState()
    {
        return currentState;
    }

    @Override
    public EnumMap<T, Long> getStateTimesMillis()
    {
        long[] millis = getStateTimesMillisArray();
        EnumMap<T, Long> map = new EnumMap<>(klass);
        for (int i = 0; i < millis.length; i++)
        {
            long ms = millis[i];
            if (ms != 0)
                map.put(klass.getEnumConstants()[i], ms);
        }
        return map;
    }

    @Override
    protected void onComplete()
    {
        currentState = COMPLETE;
    }

    private long[] getStateTimesMillisArray()
    {
        long[] millis = new long[stateTimesNanos.length];
        for (int i = 0; i < millis.length; i++)
        {
            long value = stateTimesNanos[i];
            if (value != 0)
                millis[i] = nanosToMillis(value);
        }
        return millis;
    }

    protected void updateState(T state)
    {
        if (maybeUpdateState(state).isRejected())
            throw new IllegalStateException("State went backwards; current=" + klass.getEnumConstants()[currentState] + ", desired=" + state);
    }

    protected UpdateType maybeUpdateState(T state)
    {
        int currentState = this.currentState;
        if (currentState == COMPLETE)
            return UpdateType.ALREADY_COMPLETED;
        if (currentState == state.ordinal())
            return UpdateType.NO_CHANGE;
        if (currentState > state.ordinal())
            return UpdateType.LARGER_STATE_SEEN;
        long now = clock.nanoTime();
        stateTimesNanos[this.currentState = state.ordinal()] = now;
        lastUpdatedAtNs = now;
        return UpdateType.ACCEPTED;
    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.cassandra.security;

import java.io.IOException;
import java.lang.reflect.Constructor;
import java.security.InvalidAlgorithmParameterException;
import java.security.InvalidKeyException;
import java.security.Key;
import java.security.NoSuchAlgorithmException;
import java.security.SecureRandom;
import java.util.Arrays;
import java.util.concurrent.CompletionException;
import javax.crypto.Cipher;
import javax.crypto.NoSuchPaddingException;
import javax.crypto.spec.IvParameterSpec;

import com.google.common.annotations.VisibleForTesting;

import com.github.benmanes.caffeine.cache.Caffeine;
import com.github.benmanes.caffeine.cache.LoadingCache;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import io.netty.util.concurrent.FastThreadLocal;
import org.apache.cassandra.concurrent.ImmediateExecutor;
import org.apache.cassandra.config.TransparentDataEncryptionOptions;

/**
 * A factory for loading encryption keys from {@link KeyProvider} instances.
 * Maintains a cache of loaded keys to avoid invoking the key provider on every call.
 */
public class CipherFactory
{
    private final Logger logger = LoggerFactory.getLogger(CipherFactory.class);

    /**
     * Keep around thread local instances of Cipher as they are quite expensive to instantiate (@code Cipher#getInstance).
     * Bonus points if you can avoid calling (@code Cipher#init); hence, the point of the supporting struct
     * for caching Cipher instances.
     */
    private static final FastThreadLocal<CachedCipher> cipherThreadLocal = new FastThreadLocal<>();

    private final SecureRandom secureRandom;
    private final LoadingCache<String, Key> cache;
    private final int ivLength;
    private final KeyProvider keyProvider;

    public CipherFactory(TransparentDataEncryptionOptions options)
    {
        logger.info("initializing CipherFactory");
        ivLength = options.iv_length;

        try
        {
            secureRandom = SecureRandom.getInstance("SHA1PRNG");
            Class<KeyProvider> keyProviderClass = (Class<KeyProvider>)Class.forName(options.key_provider.class_name);
            Constructor ctor = keyProviderClass.getConstructor(TransparentDataEncryptionOptions.class);
            keyProvider = (KeyProvider)ctor.newInstance(options);
        }
        catch (Exception e)
        {
            throw new RuntimeException("couldn't load cipher factory", e);
        }

        cache = Caffeine.newBuilder() // by default cache is unbounded
                .maximumSize(64) // a value large enough that we should never even get close (so nothing gets evicted)
                .executor(ImmediateExecutor.INSTANCE)
                .removalListener((key, value, cause) ->
                {
                    // maybe reload the key? (to avoid the reload being on the user's dime)
                    logger.info("key {} removed from cipher key cache", key);
                })
                .build(alias ->
                       {
                           logger.info("loading secret key for alias {}", alias);
                           return keyProvider.getSecretKey(alias);
                       });
    }

    public Cipher getEncryptor(String transformation, String keyAlias) throws IOException
    {
        byte[] iv = new byte[ivLength];
        secureRandom.nextBytes(iv);
        return buildCipher(transformation, keyAlias, iv, Cipher.ENCRYPT_MODE);
    }

    public Cipher getDecryptor(String transformation, String keyAlias, byte[] iv) throws IOException
    {
        assert iv != null && iv.length > 0 : "trying to decrypt, but the initialization vector is empty";
        return buildCipher(transformation, keyAlias, iv, Cipher.DECRYPT_MODE);
    }

    @VisibleForTesting
    Cipher buildCipher(String transformation, String keyAlias, byte[] iv, int cipherMode) throws IOException
    {
        try
        {
            CachedCipher cachedCipher = cipherThreadLocal.get();
            if (cachedCipher != null)
            {
                Cipher cipher = cachedCipher.cipher;
                // rigorous checks to make sure we've absolutely got the correct instance (with correct alg/key/iv/...)
                if (cachedCipher.mode == cipherMode && cipher.getAlgorithm().equals(transformation)
                    && cachedCipher.keyAlias.equals(keyAlias) && Arrays.equals(cipher.getIV(), iv))
                    return cipher;
            }

            Key key = retrieveKey(keyAlias);
            Cipher cipher = Cipher.getInstance(transformation);
            cipher.init(cipherMode, key, new IvParameterSpec(iv));
            cipherThreadLocal.set(new CachedCipher(cipherMode, keyAlias, cipher));
            return cipher;
        }
        catch (NoSuchAlgorithmException | NoSuchPaddingException | InvalidAlgorithmParameterException | InvalidKeyException e)
        {
            logger.error("could not build cipher", e);
            throw new IOException("cannot load cipher", e);
        }
    }

    private Key retrieveKey(String keyAlias) throws IOException
    {
        try
        {
            return cache.get(keyAlias);
        }
        catch (CompletionException e)
        {
            if (e.getCause() instanceof IOException)
                throw (IOException)e.getCause();
            throw new IOException("failed to load key from cache: " + keyAlias, e);
        }
    }

    /**
     * A simple struct to use with the thread local caching of Cipher as we can't get the mode (encrypt/decrypt) nor
     * key_alias (or key!) from the Cipher itself to use for comparisons
     */
    private static class CachedCipher
    {
        public final int mode;
        public final String keyAlias;
        public final Cipher cipher;

        private CachedCipher(int mode, String keyAlias, Cipher cipher)
        {
            this.mode = mode;
            this.keyAlias = keyAlias;
            this.cipher = cipher;
        }
    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.cassandra.index.sasi.disk;

import java.nio.ByteBuffer;
import java.util.*;

import org.apache.cassandra.utils.AbstractIterator;
import org.apache.cassandra.utils.Pair;

import com.carrotsearch.hppc.LongHashSet;
import com.carrotsearch.hppc.LongSet;
import com.carrotsearch.hppc.cursors.LongCursor;

public class DynamicTokenTreeBuilder extends AbstractTokenTreeBuilder
{
    private final SortedMap<Long, LongSet> tokens = new TreeMap<>();


    public DynamicTokenTreeBuilder()
    {}

    public DynamicTokenTreeBuilder(TokenTreeBuilder data)
    {
        add(data);
    }

    public DynamicTokenTreeBuilder(SortedMap<Long, LongSet> data)
    {
        add(data);
    }

    public void add(Long token, long keyPosition)
    {
        LongSet found = tokens.get(token);
        if (found == null)
            tokens.put(token, (found = new LongHashSet(2)));

        found.add(keyPosition);
    }

    public void add(Iterator<Pair<Long, LongSet>> data)
    {
        while (data.hasNext())
        {
            Pair<Long, LongSet> entry = data.next();
            for (LongCursor l : entry.right)
                add(entry.left, l.value);
        }
    }

    public void add(SortedMap<Long, LongSet> data)
    {
        for (Map.Entry<Long, LongSet> newEntry : data.entrySet())
        {
            LongSet found = tokens.get(newEntry.getKey());
            if (found == null)
                tokens.put(newEntry.getKey(), (found = new LongHashSet(4)));

            for (LongCursor offset : newEntry.getValue())
                found.add(offset.value);
        }
    }

    public Iterator<Pair<Long, LongSet>> iterator()
    {
        final Iterator<Map.Entry<Long, LongSet>> iterator = tokens.entrySet().iterator();
        return new AbstractIterator<Pair<Long, LongSet>>()
        {
            protected Pair<Long, LongSet> computeNext()
            {
                if (!iterator.hasNext())
                    return endOfData();

                Map.Entry<Long, LongSet> entry = iterator.next();
                return Pair.create(entry.getKey(), entry.getValue());
            }
        };
    }

    public boolean isEmpty()
    {
        return tokens.size() == 0;
    }

    protected void constructTree()
    {
        tokenCount = tokens.size();
        treeMinToken = tokens.firstKey();
        treeMaxToken = tokens.lastKey();
        numBlocks = 1;

        // special case the tree that only has a single block in it (so we don't create a useless root)
        if (tokenCount <= TOKENS_PER_BLOCK)
        {
            leftmostLeaf = new DynamicLeaf(tokens);
            rightmostLeaf = leftmostLeaf;
            root = leftmostLeaf;
        }
        else
        {
            root = new InteriorNode();
            rightmostParent = (InteriorNode) root;

            int i = 0;
            Leaf lastLeaf = null;
            Long firstToken = tokens.firstKey();
            Long finalToken = tokens.lastKey();
            Long lastToken;
            for (Long token : tokens.keySet())
            {
                if (i == 0 || (i % TOKENS_PER_BLOCK != 0 && i != (tokenCount - 1)))
                {
                    i++;
                    continue;
                }

                lastToken = token;
                Leaf leaf = (i != (tokenCount - 1) || token.equals(finalToken)) ?
                        new DynamicLeaf(tokens.subMap(firstToken, lastToken)) : new DynamicLeaf(tokens.tailMap(firstToken));

                if (i == TOKENS_PER_BLOCK)
                    leftmostLeaf = leaf;
                else
                    lastLeaf.next = leaf;

                rightmostParent.add(leaf);
                lastLeaf = leaf;
                rightmostLeaf = leaf;
                firstToken = lastToken;
                i++;
                numBlocks++;

                if (token.equals(finalToken))
                {
                    Leaf finalLeaf = new DynamicLeaf(tokens.tailMap(token));
                    lastLeaf.next = finalLeaf;
                    rightmostParent.add(finalLeaf);
                    rightmostLeaf = finalLeaf;
                    numBlocks++;
                }
            }

        }
    }

    private class DynamicLeaf extends Leaf
    {
        private final SortedMap<Long, LongSet> tokens;

        DynamicLeaf(SortedMap<Long, LongSet> data)
        {
            super(data.firstKey(), data.lastKey());
            tokens = data;
        }

        public int tokenCount()
        {
            return tokens.size();
        }

        public boolean isSerializable()
        {
            return true;
        }

        protected void serializeData(ByteBuffer buf)
        {
            for (Map.Entry<Long, LongSet> entry : tokens.entrySet())
                createEntry(entry.getKey(), entry.getValue()).serialize(buf);
        }

    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.cassandra.hints;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Collections;
import java.util.Iterator;
import java.util.Queue;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.atomic.AtomicLong;
import java.util.zip.CRC32;

import org.apache.cassandra.io.util.DataOutputBuffer;
import org.apache.cassandra.io.util.DataOutputBufferFixed;
import org.apache.cassandra.net.MessagingService;
import org.apache.cassandra.utils.AbstractIterator;
import org.apache.cassandra.utils.concurrent.OpOrder;
import org.apache.cassandra.utils.memory.MemoryUtil;

import static org.apache.cassandra.utils.FBUtilities.updateChecksum;
import static org.apache.cassandra.utils.FBUtilities.updateChecksumInt;

/**
 * A shared buffer that temporarily holds the serialized hints before they are flushed to disk.
 *
 * Consists of :
 * - a ByteBuffer holding the serialized hints (length, length checksum and total checksum included)
 * - a pointer to the current allocation offset
 * - an {@link OpOrder} appendOrder for {@link HintsWriteExecutor} to wait on for all writes completion
 * - a map of (host id -> offset queue) for the hints written
 *
 * It's possible to write a single hint for two or more hosts at the same time, in which case the same offset will be put
 * into two or more offset queues.
 */
final class HintsBuffer
{
    // hint entry overhead in bytes (int length, int length checksum, int body checksum)
    static final int ENTRY_OVERHEAD_SIZE = 12;

    private final ByteBuffer slab; // the underlying backing ByteBuffer for all the serialized hints
    private final AtomicLong position; // the position in the slab that we currently allocate from

    private final ConcurrentMap<UUID, Queue<Integer>> offsets;
    private final OpOrder appendOrder;

    private HintsBuffer(ByteBuffer slab)
    {
        this.slab = slab;

        position = new AtomicLong();
        offsets = new ConcurrentHashMap<>();
        appendOrder = new OpOrder();
    }

    static HintsBuffer create(int slabSize)
    {
        return new HintsBuffer(ByteBuffer.allocateDirect(slabSize));
    }

    boolean isClosed()
    {
        return position.get() < 0;
    }

    int capacity()
    {
        return slab.capacity();
    }

    int remaining()
    {
        long pos = position.get();
        return (int) (pos < 0 ? 0 : Math.max(0, capacity() - pos));
    }

    HintsBuffer recycle()
    {
        slab.clear();
        return new HintsBuffer(slab);
    }

    void free()
    {
        MemoryUtil.clean(slab);
    }

    /**
     * Wait for any appends started before this method was called.
     */
    void waitForModifications()
    {
        appendOrder.awaitNewBarrier(); // issue a barrier and wait for it
    }

    Set<UUID> hostIds()
    {
        return offsets.keySet();
    }

    /**
     * Coverts the queue of offsets for the selected host id into an iterator of hints encoded as ByteBuffers.
     */
    Iterator<ByteBuffer> consumingHintsIterator(UUID hostId)
    {
        final Queue<Integer> bufferOffsets = offsets.get(hostId);

        if (bufferOffsets == null)
            return Collections.emptyIterator();

        return new AbstractIterator<>()
        {
            private final ByteBuffer flyweight = slab.duplicate();

            protected ByteBuffer computeNext()
            {
                Integer offset = bufferOffsets.poll();

                if (offset == null)
                    return endOfData();

                int totalSize = slab.getInt(offset) + ENTRY_OVERHEAD_SIZE;

                return flyweight.clear().position(offset).limit(offset + totalSize);
            }
        };
    }

    Allocation allocate(int hintSize)
    {
        int totalSize = hintSize + ENTRY_OVERHEAD_SIZE;

        if (totalSize > slab.capacity() / 2)
        {
            throw new IllegalArgumentException(String.format("Hint of %s bytes is too large - the maximum size is %s",
                                                             hintSize,
                                                             slab.capacity() / 2));
        }

        OpOrder.Group opGroup = appendOrder.start(); // will eventually be closed by the receiver of the allocation
        try
        {
            return allocate(totalSize, opGroup);
        }
        catch (Throwable t)
        {
            opGroup.close();
            throw t;
        }
    }

    private Allocation allocate(int totalSize, OpOrder.Group opGroup)
    {
        int offset = allocateBytes(totalSize);
        if (offset < 0)
        {
            opGroup.close();
            return null;
        }
        return new Allocation(offset, totalSize, opGroup);
    }

    /**
     * Allocate bytes in the segment, or return -1 if not enough space. Method ensures that marker bytes
     * for each allocation (i.e. offset of its end) is written as a 32 bit integer at its beginning, and
     * that these marker bytes are always written sequentially. In other words, if allocation A has a lower
     * starting offset than allocation B, A's marker will always be written before the offset for B is returned.
     *
     * `allocateOffset` consists of two integers:
     *    64                 32                0
     *    | (i32) inProgress | (i32) writtenTo |
     *
     *  If inProgress bytes are not zeroes, they contain an unwritten offset. Before allocating any bytes,
     *  inProgresss bytes need to be written at the writtenTo location in the target buffer.
     */
    private int allocateBytes(int totalSize)
    {
        long prev = position.getAndAdd(totalSize);

        if (prev < 0) // the slab has been 'closed'
            return -1;

        if ((prev + totalSize) > slab.capacity())
        {
            position.set(Long.MIN_VALUE); // mark the slab as no longer allocating if we've exceeded its capacity
            return -1;
        }

        return (int)prev;
    }

    private void put(UUID hostId, int offset)
    {
        // we intentionally don't just return offsets.computeIfAbsent() because it's expensive compared to simple get(),
        // and the method is on a really hot path
        Queue<Integer> queue = offsets.get(hostId);
        if (queue == null)
            queue = offsets.computeIfAbsent(hostId, (id) -> new ConcurrentLinkedQueue<>());
        queue.offer(offset);
    }

    /**
     * A placeholder for hint serialization. Should always be used in a try-with-resources block.
     */
    final class Allocation implements AutoCloseable
    {
        private final Integer offset;
        private final int totalSize;
        private final OpOrder.Group opGroup;

        Allocation(int offset, int totalSize, OpOrder.Group opGroup)
        {
            this.offset = offset;
            this.totalSize = totalSize;
            this.opGroup = opGroup;
        }

        void write(Iterable<UUID> hostIds, Hint hint)
        {
            write(hint);
            for (UUID hostId : hostIds)
                put(hostId, offset);
        }

        public void close()
        {
            opGroup.close();
        }

        private void write(Hint hint)
        {
            ByteBuffer buffer = slab.duplicate().position(offset).limit(offset + totalSize);
            CRC32 crc = new CRC32();
            int hintSize = totalSize - ENTRY_OVERHEAD_SIZE;
            try (DataOutputBuffer dop = new DataOutputBufferFixed(buffer))
            {
                dop.writeInt(hintSize);
                updateChecksumInt(crc, hintSize);
                dop.writeInt((int) crc.getValue());

                Hint.serializer.serialize(hint, dop, MessagingService.current_version);
                updateChecksum(crc, buffer, buffer.position() - hintSize, hintSize);
                dop.writeInt((int) crc.getValue());
            }
            catch (IOException e)
            {
                throw new AssertionError(); // cannot happen
            }
        }
    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.cassandra.streaming.async;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

import com.google.common.annotations.VisibleForTesting;

import io.netty.channel.Channel;
import io.netty.channel.ChannelPipeline;
import io.netty.channel.EventLoop;
import io.netty.util.concurrent.Future; // checkstyle: permit this import
import org.apache.cassandra.net.ConnectionCategory;
import org.apache.cassandra.net.MessagingService;
import org.apache.cassandra.net.OutboundConnectionInitiator.Result;
import org.apache.cassandra.net.OutboundConnectionInitiator.Result.StreamingSuccess;
import org.apache.cassandra.net.OutboundConnectionSettings;
import org.apache.cassandra.streaming.StreamingChannel;

import static org.apache.cassandra.locator.InetAddressAndPort.getByAddress;
import static org.apache.cassandra.net.InternodeConnectionUtils.isSSLError;
import static org.apache.cassandra.net.OutboundConnectionInitiator.initiateStreaming;
import static org.apache.cassandra.net.OutboundConnectionInitiator.SslFallbackConnectionType;
import static org.apache.cassandra.net.OutboundConnectionInitiator.SslFallbackConnectionType.SERVER_CONFIG;

public class NettyStreamingConnectionFactory implements StreamingChannel.Factory
{
    @VisibleForTesting
    public static int MAX_CONNECT_ATTEMPTS = 3;

    public static NettyStreamingChannel connect(OutboundConnectionSettings template, int messagingVersion, StreamingChannel.Kind kind) throws IOException
    {
        EventLoop eventLoop = MessagingService.instance().socketFactory.outboundStreamingGroup().next();
        OutboundConnectionSettings settings = template.withDefaults(ConnectionCategory.STREAMING);
        List<SslFallbackConnectionType> sslFallbacks = settings.withEncryption() && settings.encryption.getOptional()
                                                       ? Arrays.asList(SslFallbackConnectionType.values())
                                                       : Collections.singletonList(SERVER_CONFIG);

        Throwable cause = null;
        for (final SslFallbackConnectionType sslFallbackConnectionType : sslFallbacks)
        {
            for (int i = 0; i < MAX_CONNECT_ATTEMPTS; i++)
            {
                Future<Result<StreamingSuccess>> result = initiateStreaming(eventLoop, settings, sslFallbackConnectionType);
                result.awaitUninterruptibly(); // initiate has its own timeout, so this is "guaranteed" to return relatively promptly
                if (result.isSuccess())
                {
                    Channel channel = result.getNow().success().channel;
                    NettyStreamingChannel streamingChannel = new NettyStreamingChannel(channel, kind);
                    if (kind == StreamingChannel.Kind.CONTROL)
                    {
                        ChannelPipeline pipeline = channel.pipeline();
                        pipeline.addLast("stream", streamingChannel);
                    }
                    return streamingChannel;
                }
                cause = result.cause();
            }
            if (!isSSLError(cause))
            {
                // Fallback only when the error is SSL related, otherwise retries are exhausted, so fail
                break;
            }
        }
        throw new IOException("failed to connect to " + template.to + " for streaming data", cause);
    }

    @Override
    public StreamingChannel create(InetSocketAddress to, int messagingVersion, StreamingChannel.Kind kind) throws IOException
    {
        return connect(new OutboundConnectionSettings(getByAddress(to)), messagingVersion, kind);
    }

    @Override
    public StreamingChannel create(InetSocketAddress to,
                                   InetSocketAddress preferred,
                                   int messagingVersion,
                                   StreamingChannel.Kind kind) throws IOException
    {
        return connect(new OutboundConnectionSettings(getByAddress(to), getByAddress(preferred)), messagingVersion, kind);
    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.cassandra.schema;

import java.util.Set;

import org.apache.cassandra.diag.DiagnosticEventService;
import org.apache.cassandra.locator.InetAddressAndPort;
import org.apache.cassandra.schema.SchemaAnnouncementEvent.SchemaAnnouncementEventType;

final class SchemaAnnouncementDiagnostics
{
    private static final DiagnosticEventService service = DiagnosticEventService.instance();

    private SchemaAnnouncementDiagnostics()
    {
    }

    static void schemaMutationsAnnounced(Set<InetAddressAndPort> schemaDestinationEndpoints, Set<InetAddressAndPort> schemaEndpointsIgnored)
    {
        if (isEnabled(SchemaAnnouncementEventType.SCHEMA_MUTATIONS_ANNOUNCED))
            service.publish(new SchemaAnnouncementEvent(SchemaAnnouncementEventType.SCHEMA_MUTATIONS_ANNOUNCED,
                                                        schemaDestinationEndpoints, schemaEndpointsIgnored, null, null));
    }

    public static void schemataMutationsReceived(InetAddressAndPort from)
    {
        if (isEnabled(SchemaAnnouncementEventType.SCHEMA_MUTATIONS_RECEIVED))
            service.publish(new SchemaAnnouncementEvent(SchemaAnnouncementEventType.SCHEMA_MUTATIONS_RECEIVED,
                                                        null, null, null, from));
    }

    static void schemaTransformationAnnounced(Set<InetAddressAndPort> schemaDestinationEndpoints, Set<InetAddressAndPort> schemaEndpointsIgnored, SchemaTransformation transformation)
    {
        if (isEnabled(SchemaAnnouncementEventType.SCHEMA_TRANSFORMATION_ANNOUNCED))
            service.publish(new SchemaAnnouncementEvent(SchemaAnnouncementEventType.SCHEMA_TRANSFORMATION_ANNOUNCED,
                                                        schemaDestinationEndpoints, schemaEndpointsIgnored, transformation, null));
    }

    private static boolean isEnabled(SchemaAnnouncementEventType type)
    {
        return service.isEnabled(SchemaAnnouncementEvent.class, type);
    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.cassandra.schema;

import java.util.UUID;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.cassandra.net.IVerbHandler;
import org.apache.cassandra.net.Message;
import org.apache.cassandra.net.MessagingService;
import org.apache.cassandra.net.NoPayload;

public final class SchemaVersionVerbHandler implements IVerbHandler<NoPayload>
{
    public static final SchemaVersionVerbHandler instance = new SchemaVersionVerbHandler();

    private final Logger logger = LoggerFactory.getLogger(SchemaVersionVerbHandler.class);

    public void doVerb(Message<NoPayload> message)
    {
        logger.trace("Received schema version request from {}", message.from());
        Message<UUID> response = message.responseWith(Schema.instance.getVersion());
        MessagingService.instance().send(response, message.from());
    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.cassandra.locator;

import java.util.Comparator;
import java.util.HashSet;
import java.util.Set;

import org.apache.cassandra.exceptions.ConfigurationException;
import org.apache.cassandra.tcm.ClusterMetadata;
import org.apache.cassandra.tcm.membership.Location;
import org.apache.cassandra.utils.Sortable;

public class SnitchAdapter implements InitialLocationProvider, NodeProximity, NodeAddressConfig
{
    public final IEndpointSnitch snitch;

    public SnitchAdapter(IEndpointSnitch snitch)
    {
        this.snitch = snitch;
    }

    @Override
    public Location initialLocation()
    {
        return new Location(snitch.getLocalDatacenter(), snitch.getLocalRack());
    }

    @Override
    public void validate(ClusterMetadata metadata)
    {
        Set<String> datacenters = metadata.directory.allDatacenterRacks().keySet();
        Set<String> racks = new HashSet<>();
        for (String dc : datacenters)
            racks.addAll(metadata.directory.datacenterRacks(dc).keySet());
        if (!snitch.validate(datacenters, racks))
            throw new ConfigurationException("Initial location provider rejected registration location, " +
                                             "please check the system log for errors");
    }

    @Override
    public <C extends ReplicaCollection<? extends C>> C sortedByProximity(InetAddressAndPort address, C addresses)
    {
        return snitch.sortedByProximity(address, addresses);
    }

    @Override
    public int compareEndpoints(InetAddressAndPort target, Replica r1, Replica r2)
    {
        return snitch.compareEndpoints(target, r1, r2);
    }

    @Override
    public boolean isWorthMergingForRangeQuery(ReplicaCollection<?> merged, ReplicaCollection<?> l1, ReplicaCollection<?> l2)
    {
        return snitch.isWorthMergingForRangeQuery(merged, l1, l2);
    }

    @Override
    public void configureAddresses()
    {
        snitch.configureAddresses();
    }

    @Override
    public boolean preferLocalConnections()
    {
        return snitch.preferLocalConnections();
    }

    @Override
    public boolean supportCompareByEndpoint()
    {
        return snitch.supportCompareByEndpoint();
    }

    @Override
    public <C extends Sortable<? extends Endpoint, ? extends C>> Comparator<Endpoint> endpointComparator(InetAddressAndPort address, C addresses)
    {
        return snitch.endpointComparator(address, addresses);
    }
}
<<<<<<<<DIVIDER>>>>>>>>
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.cassandra.io.util;

import java.io.EOFException;
import java.io.IOException;

import org.apache.cassandra.db.TypeSizes;
import org.apache.cassandra.utils.Throwables;

/**
 * DataInput that also stores the raw inputs into an output buffer
 * This is useful for storing serialized buffers as they are deserialized.
 *
 * Note: If a non-zero limit is included it is important to for callers to check {@link #isLimitReached()}
 * before using the tee buffer as it could be cropped.
 */
public class TeeDataInputPlus implements DataInputPlus
{
    private final DataInputPlus source;
    private final DataOutputPlus teeBuffer;

    private final long limit;
    private boolean limitReached;

    public TeeDataInputPlus(DataInputPlus source, DataOutputPlus teeBuffer)
    {
        this(source, teeBuffer, 0);
    }

    public TeeDataInputPlus(DataInputPlus source, DataOutputPlus teeBuffer, long limit)
    {
        assert source != null && teeBuffer != null;
        this.source = source;
        this.teeBuffer = teeBuffer;
        this.limit = limit;
        this.limitReached = false;
    }

    private void maybeWrite(int length, Throwables.DiscreteAction<IOException> writeAction) throws IOException
    {
        if (limit <= 0 || (!limitReached && (teeBuffer.position() + length) < limit))
            writeAction.perform();
        else
            limitReached = true;
    }

    @Override
    public void readFully(byte[] bytes) throws IOException
    {
        source.readFully(bytes);
        maybeWrite(bytes.length, () -> teeBuffer.write(bytes));
    }

    @Override
    public void readFully(byte[] bytes, int offset, int length) throws IOException
    {
        source.readFully(bytes, offset, length);
        maybeWrite(length, () -> teeBuffer.write(bytes, offset, length));
    }

    @Override
    public int skipBytes(int n) throws IOException
    {
        for (int i = 0; i < n; i++)
        {
            try
            {
                byte v = source.readByte();
                maybeWrite(TypeSizes.BYTE_SIZE, () -> teeBuffer.writeByte(v));
            }
            catch (EOFException eof)
            {
                return i;
            }
        }
        return n;
    }

    @Override
    public boolean readBoolean() throws IOException
    {
        boolean v = source.readBoolean();
        maybeWrite(TypeSizes.BOOL_SIZE, () -> teeBuffer.writeBoolean(v));
        return v;
    }

    @Override
    public byte readByte() throws IOException
    {
        byte v = source.readByte();
        maybeWrite(TypeSizes.BYTE_SIZE, () -> teeBuffer.writeByte(v));
        return v;
    }

    @Override
    public int readUnsignedByte() throws IOException
    {
        int v = source.readUnsignedByte();
        maybeWrite(TypeSizes.BYTE_SIZE, () -> teeBuffer.writeByte(v));
        return v;
    }

    @Override
    public short readShort() throws IOException
    {
        short v = source.readShort();
        maybeWrite(TypeSizes.SHORT_SIZE, () -> teeBuffer.writeShort(v));
        return v;
    }

    @Override
    public int readUnsignedShort() throws IOException
    {
        int v = source.readUnsignedShort();
        maybeWrite(TypeSizes.SHORT_SIZE, () -> teeBuffer.writeShort(v));
        return v;
    }

    @Override
    public char readChar() throws IOException
    {
        char v = source.readChar();
        maybeWrite(TypeSizes.BYTE_SIZE, () -> teeBuffer.writeChar(v));
        return v;
    }

    @Override
    public int readInt() throws IOException
    {
        int v = source.readInt();
        maybeWrite(TypeSizes.INT_SIZE, () -> teeBuffer.writeInt(v));
        return v;
    }

    @Override
    public long readLong() throws IOException
    {
        long v = source.readLong();
        maybeWrite(TypeSizes.LONG_SIZE, () -> teeBuffer.writeLong(v));
        return v;
    }

    @Override
    public float readFloat() throws IOException
    {
        float v = source.readFloat();
        maybeWrite(TypeSizes.FLOAT_SIZE, () -> teeBuffer.writeFloat(v));
        return v;
    }

    @Override
    public double readDouble() throws IOException
    {
        double v = source.readDouble();
        maybeWrite(TypeSizes.DOUBLE_SIZE, () -> teeBuffer.writeDouble(v));
        return v;
    }

    @Override
    public String readLine() throws IOException
    {
        //This one isn't safe since we know the actual line termination type
        throw new UnsupportedOperationException();
    }

    @Override
    public String readUTF() throws IOException
    {
        String v = source.readUTF();
        maybeWrite(TypeSizes.sizeof(v), () -> teeBuffer.writeUTF(v));
        return v;
    }

    @Override
    public long readVInt() throws IOException
    {
        long v = source.readVInt();
        maybeWrite(TypeSizes.sizeofVInt(v), () -> teeBuffer.writeVInt(v));
        return v;
    }

    @Override
    public long readUnsignedVInt() throws IOException
    {
        long v = source.readUnsignedVInt();
        maybeWrite(TypeSizes.sizeofUnsignedVInt(v), () -> teeBuffer.writeUnsignedVInt(v));
        return v;
    }

    @Override
    public void skipBytesFully(int n) throws IOException
    {
        source.skipBytesFully(n);
        maybeWrite(n, () -> {
            for (int i = 0; i < n; i++)
                teeBuffer.writeByte(0);
        });
    }

    /**
     * Used to detect if the teeBuffer hit the supplied limit.
     * If true this means the teeBuffer does not contain the full input.
     */
    public boolean isLimitReached()
    {
        return limitReached;
    }
}
